epoch: 0 train loss: 3.914549986521403
epoch: 0 val loss: 3.8615820472692297
epoch: 1 train loss: 3.7027758757273355
epoch: 1 val loss: 3.6421220688535767
epoch: 2 train loss: 3.3803269068400064
epoch: 2 val loss: 3.281936385587917
epoch: 3 train loss: 2.814069906870524
epoch: 3 val loss: 2.760487911838982
epoch: 4 train loss: 2.2249956130981445
epoch: 4 val loss: 2.520717348531691
epoch: 5 train loss: 1.7546489636103313
epoch: 5 val loss: 2.2240550003608868
epoch: 6 train loss: 1.1934399207433064
epoch: 6 val loss: 2.049227131344779
epoch: 7 train loss: 0.9216431379318237
epoch: 7 val loss: 1.912485789486233
epoch: 8 train loss: 0.7558632095654806
epoch: 8 val loss: 1.4091798247500709
epoch: 9 train loss: 0.7102118333180746
epoch: 9 val loss: 1.289758472268156
epoch: 10 train loss: 0.5990107357501984
epoch: 10 val loss: 1.5555357844364872
epoch: 11 train loss: 0.6258313159147898
epoch: 11 val loss: 1.7104575066517689
train loss went up, stopping now
moore_a best f1: 0.6484728959010714 best link: 4.909092903137207
allen_d best f1: 0.45161290322580644 best link: -7.909608840942383
best threshold: -5.208960304229354
201 data points
119 merged performed
test f1 on blum_a: 0.05651968751638442
471 data points
333 merged performed
test f1 on jones_s: 0.35447552091293494
31 data points
12 merged performed
test f1 on robinson_h: 0.5128205128205128
test f1: 0.30793857374994404

real	7m53.561s
user	8m54.111s
sys	0m14.436s
